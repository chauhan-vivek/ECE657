{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ea3ac94",
   "metadata": {},
   "source": [
    "## Assignment - 1 : Implementing Backpropagation Algorithm\n",
    "\n",
    "**Date: 6th June 2022**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d7df36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries\n",
    "import time\n",
    "from csv import reader\n",
    "from math import exp\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65be6374",
   "metadata": {},
   "source": [
    "#### Loading and cleaning file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abc21335",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_file(file):\n",
    "    \"\"\"\n",
    "    This function accepts the filename agrument which is the csv file that is to be read\n",
    "    and outputs the dataset\n",
    "    \"\"\"\n",
    "    df = [] #empty list to store the dataset row wise\n",
    "    with open(file, 'r') as f: #file opened in read mode only\n",
    "        file_read = reader(f)\n",
    "        for row in file_read:\n",
    "            if not row:\n",
    "                #if the file has reached the end or the row is blank\n",
    "                continue\n",
    "            df.append(row)\n",
    "    return df\n",
    "\n",
    "# Type casts string column to float as normalized pixel values are expected to be float\n",
    "def preprocessing_file(df, col):\n",
    "    \"\"\"\n",
    "    This function accepts the dataset returned by load_csv function and column as argument to strip\n",
    "    any blank space at the end of each cell and casts it into float\n",
    "    \"\"\"\n",
    "    for row in df:\n",
    "        row[col] = float(row[col].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cd03b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = r'train_data.csv'\n",
    "x = read_csv_file(filename) #loading the data file as csv into list\n",
    "for i in range(len(x[0])): #as range is indexed from 0 \n",
    "    preprocessing_file(x, i) # strip and type cast string to float\n",
    "    \n",
    "filename_2 = r'train_labels.csv'\n",
    "y = read_csv_file(filename_2) #loading the data labels file as csv into list\n",
    "for i in range(len(y[0])):\n",
    "    preprocessing_file(y, i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86ac40d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to handle 'U32 safe' error \n",
    "x = np.array(x, dtype=np.float32) \n",
    "y = np.array(y, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23c8368b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train_data is 24754 train_labels is 24754\n"
     ]
    }
   ],
   "source": [
    "print(f'Length of train_data is {len(x)} train_labels is {len(y)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e994c894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Training dataset is : 19803 , 19803\n",
      "Length of Training dataset is : 4951 , 4951\n"
     ]
    }
   ],
   "source": [
    "#using the train_test_split function from scikit learn to split our dataset into training and validation\n",
    "#data set in the ratio of 80:20 \n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.20, random_state=42)\n",
    "print(f'Length of Training dataset is : {len(x_train)} , {len(y_train)}')\n",
    "print(f'Length of Training dataset is : {len(x_val)} , {len(y_val)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c322c1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    \"\"\"\n",
    "    class consists of methods for forward pass , backpropagation , activation function(sigmoid) \n",
    "    and other required methods. We pass dfault hyperparameters of our choice to initialize the methods. \n",
    "    \"\"\"\n",
    "    def __init__(self, shape_list, ep=15, learning_rate=0.1):\n",
    "        self.shapes = shape_list # list that specifices the input vector size, number of neurons in hidden layer & output size  \n",
    "        self.epoch = ep\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # to store all parameters in the NN\n",
    "        self.parameters = self.network_initialization()\n",
    "\n",
    "    def sigmoid(self, x, d=False):\n",
    "        if d:\n",
    "            return (np.exp(-x))/((np.exp(-x)+1)**2)\n",
    "        return 1/(1 + np.exp(-x))\n",
    "\n",
    "    def softmax(self, x, d=False):\n",
    "        # Numerically stable with large exponentials\n",
    "        exps = np.exp(x - x.max())\n",
    "        if d:\n",
    "            return exps / np.sum(exps, axis=0) * (1 - exps / np.sum(exps, axis=0))\n",
    "        return exps / np.sum(exps, axis=0)\n",
    "\n",
    "    def network_initialization(self):\n",
    "        # number of neurons in each layer from sizes parameter passed\n",
    "        ip_l=self.shapes[0]\n",
    "        hd_1=self.shapes[1]\n",
    "        op_l=self.shapes[2]\n",
    "\n",
    "        parameters = {\n",
    "            'w1':np.random.randn(hd_1, ip_l) * np.sqrt(1. / hd_1),\n",
    "            'b1':np.random.randn(hd_1),\n",
    "            'w2':np.random.randn(op_l, hd_1) * np.sqrt(1. / op_l),\n",
    "            'b2':np.random.randn(op_l)\n",
    "        }\n",
    "\n",
    "        return parameters\n",
    "\n",
    "    def forwardpass(self, x_train):\n",
    "        parameters = self.parameters\n",
    "\n",
    "        # input vector\n",
    "        parameters['a0'] = x_train\n",
    "\n",
    "        # from input layer to hidden layer\n",
    "        parameters['z1'] = np.dot(parameters[\"w1\"], parameters['a0']) + parameters['b1']\n",
    "        parameters['a1'] = self.sigmoid(parameters['z1'])\n",
    "\n",
    "        # from hidden layer to output layer\n",
    "        parameters['z2'] = np.dot(parameters[\"w2\"], parameters['a1']) + parameters['b2']\n",
    "        parameters['a2'] = self.softmax(parameters['z2'])\n",
    "\n",
    "        return parameters['a2']\n",
    "\n",
    "    def backpass(self, y_train, output):\n",
    "        '''\n",
    "            this function implememts backpropagation algorithm for calculating the parameter updates\n",
    "            using chaining to update parameters.\n",
    "        '''\n",
    "        parameters = self.parameters\n",
    "        #delta \n",
    "        delta_w = {}\n",
    "\n",
    "        # Calculate the W2 update\n",
    "        error = 2 * (output - y_train) / output.shape[0] * self.softmax(parameters['z2'], d=True) #default derivative parameter is set to False\n",
    "        delta_w['w2'] = np.outer(error, parameters['a1'])\n",
    "\n",
    "        # Calculate W1 update\n",
    "        error = np.dot(parameters['w2'].T, error) * self.sigmoid(parameters['z1'], d=True) #default derivative parameter is set to False\n",
    "        delta_w['w1'] = np.outer(error, parameters['a0'])\n",
    "\n",
    "        return delta_w\n",
    "\n",
    "    def update(self, delta_in_w):\n",
    "        '''\n",
    "            Updating network parameters based on update method of Stochastic Gradient Descent.\n",
    "        '''\n",
    "        \n",
    "        for key, value in delta_in_w.items():\n",
    "            self.parameters[key] -= self.learning_rate * value\n",
    "            \n",
    "    def predict(self, x_val):\n",
    "        prediction = []\n",
    "        \n",
    "        for x in x_val:\n",
    "            output = self.forwardpass(x)\n",
    "            pred = np.argmax(output)\n",
    "            #one_hot = pd.get_dummies(pred)\n",
    "            prediction.append(pred)\n",
    "        \n",
    "        return pd.get_dummies(prediction).values\n",
    "\n",
    "    def accuracy(self, x_val, y_val):\n",
    "        '''\n",
    "        this function clculates accuracy by using the highest index value returned compared to true label\n",
    "        '''\n",
    "        predictions = []\n",
    "        \n",
    "        #using the validation dataset for calculating accuracy\n",
    "        for x, y in zip(x_val, y_val):\n",
    "            output = self.forwardpass(x)\n",
    "            pred = np.argmax(output)\n",
    "            predictions.append(pred == np.argmax(y))\n",
    "        \n",
    "        return np.mean(predictions)\n",
    "    \n",
    "    #Method to call to train the network\n",
    "    def training(self, x_train, y_train, x_val, y_val):\n",
    "        start_time = time.time()\n",
    "        for i in range(self.epoch):\n",
    "            for x,y in zip(x_train, y_train):\n",
    "                output = self.forwardpass(x)\n",
    "                delta_in_w = self.backpass(y, output)\n",
    "                self.update(delta_in_w)\n",
    "            \n",
    "            accuracy = self.accuracy(x_val, y_val)\n",
    "            #y_pred = self.predict(x_val)\n",
    "            #print(f'{y_pred}')\n",
    "            '''\n",
    "            print('Epoch: {0}, Time Spent: {1:.2f}s, Accuracy: {2:.2f}%'.format(\n",
    "                i+1, time.time() - start_time, accuracy * 100\n",
    "            ))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c59c15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(shape_list=[784, 128, 4])\n",
    "nn.training(x_train, y_train, x_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37608207",
   "metadata": {},
   "source": [
    "learning_rate_list = [0.001, 0.01, 0.1, 1]\n",
    "acc_list = []\n",
    "for l in learning_rate_list:\n",
    "    nn = NeuralNetwork(shape_list=[784, 128, 4], learning_rate = l, ep = 10)\n",
    "    nn.training(x_train, y_train, x_val, y_val)\n",
    "    acc= nn.accuracy(x_val, y_val)\n",
    "    acc_list.append(acc)\n",
    "print(acc_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8c3fdb",
   "metadata": {},
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "x = learning_rate_list\n",
    "y = acc_list \n",
    "plt.plot(x,y)\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(\"Learning rate vs Accuracy plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f51e292",
   "metadata": {},
   "outputs": [],
   "source": [
    "#result = nn.predict(x_val)\n",
    "#print(result)\n",
    "#result.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c72cb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename = 'NeuralNetwork_model.sav'\n",
    "pickle.dump(nn, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32dd083f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "#saved_nn = pickle.load(open(filename, 'rb'))\n",
    "#result = saved_nn.predict(x_val)\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b42ce89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mlp(data_file):\n",
    "    #test = pd.read_csv(data_file)\n",
    "    filename = r'train_data.csv'\n",
    "    test = read_csv_file(filename) #loading the data file as csv into list\n",
    "    for i in range(len(test[0])-1): #as range is indexed from 0 \n",
    "        preprocessing_file(test, i) # strip and type cast string to float\n",
    "    #to handle U32 'safe' error due to large matrix multiplication\n",
    "    test = np.array(test, dtype=np.float32)\n",
    "    # Load your network\n",
    "    # START\n",
    "    filename_model = r'NeuralNetwork_model.sav'\n",
    "    saved_nn = pickle.load(open(filename_model, 'rb'))\n",
    "    # END\n",
    "    \n",
    "    # Predict test set - one-hot encoded\n",
    "    y_pred = saved_nn.predict(test)\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "759b17e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = test_mlp(r'train_data.csv')\n",
    "filename = r'train_labels.csv'\n",
    "test_labels = read_csv_file(filename) #loading the data file as csv into list\n",
    "for i in range(len(test_labels[0])-1): #as range is indexed from 0 \n",
    "    preprocessing_file(test_labels, i) # strip and type cast string to float\n",
    "#test_labels = pd.read_csv(r'train_labels.csv')\n",
    "test_labels = np.array(test_labels)\n",
    "test_labels = np.array(test_labels, dtype=np.float32)\n",
    "#test_accuracy = accuracy_score(test_labels, y_pred)*100\n",
    "#print(test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e527efb5",
   "metadata": {},
   "source": [
    "#Accuracy with increased number of neurons in hidden layer \n",
    "#nn = NeuralNetwork(sizes=[784, 256, 4])\n",
    "#nn.train(x_train, y_train, x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14e93ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as numpy\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    if not (len(y_true) == len(y_pred)):\n",
    "        print('Size of predicted and true labels not equal.')\n",
    "        return 0.0\n",
    "\n",
    "    corr = 0\n",
    "    for i in range(0,len(y_true)):\n",
    "        corr += 1 if (y_true[i] == y_pred[i]).all() else 0\n",
    "\n",
    "    return corr/len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cbacd1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 99.54350811989981\n"
     ]
    }
   ],
   "source": [
    "y_pred = test_mlp('train_data.csv')\n",
    "\n",
    "test_accuracy = accuracy(test_labels, y_pred)*100\n",
    "print(f'Accuracy is {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e24448b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy is : 0.9975761248295713\n"
     ]
    }
   ],
   "source": [
    "#training accuracy\n",
    "print(f'Training accuracy is : {nn.accuracy(x_train, y_train)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291a86c7",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[1]. Retreived from URL: https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/\n",
    "\n",
    "[2]. Retreived from URL: https://github.com/casperbh96/Neural-Network-From-Scratch\n",
    "\n",
    "[3]. Retreived from URL: https://towardsdatascience.com/implementing-backpropagation-with-style-in-python-da4c2f49adb4\n",
    "\n",
    "[4] Retreived from URL: https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07076d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train_data is 24754 train_labels is 24754\n",
      "Accuracy is : 99.24456653470146\n"
     ]
    }
   ],
   "source": [
    "#How we will test your code:\n",
    "from nn import read_csv_file, preprocessing_file\n",
    "from test_mlp import test_mlp, STUDENT_NAME, STUDENT_ID\n",
    "from acc_calc import accuracy \n",
    "import numpy as np\n",
    "\n",
    "#just change the file name to be tested\n",
    "filename = r'train_labels.csv'\n",
    "test_labels = read_csv_file(filename) #loading the data labels file as csv into list\n",
    "for i in range(len(test_labels[0])-1):\n",
    "    preprocessing_file(test_labels, i) \n",
    "\n",
    "#just change the file name to be tested\n",
    "y_pred = test_mlp(r'train_data.csv')\n",
    "\n",
    "test_labels = np.array(test_labels, dtype=np.float32)\n",
    "\n",
    "test_accuracy = accuracy(test_labels, y_pred)*100\n",
    "print(f'Accuracy is : {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3296ff56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
